{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKdCg+ESNqPnFc2ILwVczD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankur334/pyspark-practice/blob/tricky-pyspark-problems/solutions/faceboopk_activity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3Mo7Pnw3r7r",
        "outputId": "2a71d669-3857-4512-d299-747ad72b090c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=5040d0b194c2b29d147dbc35f8a8ee924976337fc97115fec8f4f77026c555f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Facebook Activity Analysis"
      ],
      "metadata": {
        "id": "Wd8FTAHdLsYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ref:- https://www.datasciencewithraghav.com/2020/09/03/creating-a-automated-process-to-mimic-facebook-activity-on-your-local-database/"
      ],
      "metadata": {
        "id": "34kjIg8tWGDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Friend Request"
      ],
      "metadata": {
        "id": "B0B-0HvqWV0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.types as T\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"FriendRequests\").getOrCreate()\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = T.StructType([\n",
        "    T.StructField(\"userid\", T.IntegerType()),\n",
        "    T.StructField(\"friendid\", T.IntegerType()),\n",
        "    T.StructField(\"requested_on\", T.StringType()),\n",
        "    T.StructField(\"accepted_on\", T.StringType())\n",
        "])\n",
        "\n",
        "# Create the DataFrame using a list of rows\n",
        "data = [\n",
        "    (1, 2, \"2020-07-01\", \"2020-07-01\"),\n",
        "    (1, 3, \"2020-07-01\", \"2020-07-02\"),\n",
        "    (2, 3, \"2020-07-02\", \"2020-07-03\"),\n",
        "    (1, 4, \"2020-08-01\", \"2020-08-02\"),\n",
        "    (1, 6, \"2020-08-02\", \"2020-08-03\"),\n",
        "    (1, 8, \"2020-08-20\", \"2020-08-21\"),\n",
        "    (2, 5, \"2020-08-02\", \"2020-08-02\"),\n",
        "    (2, 7, \"2020-08-03\", \"2020-08-03\"),\n",
        "    (5, 7, \"2020-08-20\", \"2020-08-21\"),\n",
        "    (4, 6, \"2020-08-15\", \"2020-08-15\")\n",
        "]\n",
        "\n",
        "# Create the DataFrame\n",
        "friend_request_df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Display the DataFrame\n",
        "friend_request_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8mF_6xHLxbe",
        "outputId": "923968fe-bdfc-41e3-ce53-b177e08b6859"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+------------+-----------+\n",
            "|userid|friendid|requested_on|accepted_on|\n",
            "+------+--------+------------+-----------+\n",
            "|     1|       2|  2020-07-01| 2020-07-01|\n",
            "|     1|       3|  2020-07-01| 2020-07-02|\n",
            "|     2|       3|  2020-07-02| 2020-07-03|\n",
            "|     1|       4|  2020-08-01| 2020-08-02|\n",
            "|     1|       6|  2020-08-02| 2020-08-03|\n",
            "|     1|       8|  2020-08-20| 2020-08-21|\n",
            "|     2|       5|  2020-08-02| 2020-08-02|\n",
            "|     2|       7|  2020-08-03| 2020-08-03|\n",
            "|     5|       7|  2020-08-20| 2020-08-21|\n",
            "|     4|       6|  2020-08-15| 2020-08-15|\n",
            "+------+--------+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "User"
      ],
      "metadata": {
        "id": "pnSXc0onWXyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.types as T\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = T.StructType([\n",
        "    T.StructField(\"userid\", T.IntegerType()),\n",
        "    T.StructField(\"first_name\", T.StringType()),\n",
        "    T.StructField(\"last_name\", T.StringType()),\n",
        "    T.StructField(\"email\", T.StringType()),\n",
        "    T.StructField(\"gender\", T.StringType()),\n",
        "    T.StructField(\"DOB\", T.StringType()),\n",
        "    T.StructField(\"home_location\", T.StringType())\n",
        "])\n",
        "\n",
        "# Create the DataFrame using a list of rows\n",
        "data = [\n",
        "    (1, 'Major', 'Razor', 'majorrazor@gmail.com', 'M', '1983-08-05', 'Chicago'),\n",
        "    (2, 'Tim', 'Fazer', 'timfazer@gmail.com', 'F', '1983-07-01', 'Chicago'),\n",
        "    (3, 'Keynote', 'Panther', 'keynotepanther@gmail.com', 'M', '2014-12-31', 'Chicago'),\n",
        "    (4, 'Chalk', 'Mater', 'chalkmater@gmail.com', 'M', '1985-07-01', 'Miami'),\n",
        "    (5, 'Kinni', 'Man', 'kinniman@gmail.com', 'F', '1990-08-01', 'Miami'),\n",
        "    (6, 'Alum', 'Mulun', 'alummulun@gmail.com', 'M', '1986-07-01', 'Naperville'),\n",
        "    (7, 'Kanon', 'Tintin', 'kanontintin@gmail.com', 'F', '1987-12-01', 'Naperville'),\n",
        "    (8, 'Binton', 'Shine', 'bintonshine@gmail.com', 'M', '1987-08-01', 'Patna'),\n",
        "    (9, 'Bere', 'Kister', 'berekister@gmail.com', 'F', '1988-02-01', 'Delhi'),\n",
        "    (10, 'Noden', 'Paten', 'nodenpaten@gmail.com', 'F', '1984-03-01', 'Lucknow')\n",
        "]\n",
        "\n",
        "\n",
        "# Create the DataFrame\n",
        "user_df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Display the DataFrame\n",
        "user_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pn3newJMok1",
        "outputId": "c9574d10-ff23-465e-be0a-e01263fb9666"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+---------+--------------------+------+----------+-------------+\n",
            "|userid|first_name|last_name|               email|gender|       DOB|home_location|\n",
            "+------+----------+---------+--------------------+------+----------+-------------+\n",
            "|     1|     Major|    Razor|majorrazor@gmail.com|     M|1983-08-05|      Chicago|\n",
            "|     2|       Tim|    Fazer|  timfazer@gmail.com|     F|1983-07-01|      Chicago|\n",
            "|     3|   Keynote|  Panther|keynotepanther@gm...|     M|2014-12-31|      Chicago|\n",
            "|     4|     Chalk|    Mater|chalkmater@gmail.com|     M|1985-07-01|        Miami|\n",
            "|     5|     Kinni|      Man|  kinniman@gmail.com|     F|1990-08-01|        Miami|\n",
            "|     6|      Alum|    Mulun| alummulun@gmail.com|     M|1986-07-01|   Naperville|\n",
            "|     7|     Kanon|   Tintin|kanontintin@gmail...|     F|1987-12-01|   Naperville|\n",
            "|     8|    Binton|    Shine|bintonshine@gmail...|     M|1987-08-01|        Patna|\n",
            "|     9|      Bere|   Kister|berekister@gmail.com|     F|1988-02-01|        Delhi|\n",
            "|    10|     Noden|    Paten|nodenpaten@gmail.com|     F|1984-03-01|      Lucknow|\n",
            "+------+----------+---------+--------------------+------+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Which user has most friends?"
      ],
      "metadata": {
        "id": "ZSQn_tDAWdTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate friend counts for both users and friendids\n",
        "user_friend_counts = friend_request_df.groupBy(\"userid\").count().withColumnRenamed(\"count\", \"friend_count\")\n",
        "friendid_friend_counts = friend_request_df.groupBy(\"friendid\").count().withColumnRenamed(\"count\", \"friend_count\")\n",
        "\n",
        "# Combine the counts and aggregate for total friends\n",
        "combined_counts = user_friend_counts.union(friendid_friend_counts)\n",
        "most_friends_df = combined_counts.groupBy(\"userid\").sum(\"friend_count\") \\\n",
        "                                  .orderBy(\"sum(friend_count)\", ascending=False) \\\n",
        "                                  .withColumnRenamed(\"sum(friend_count)\", \"total_friends\") \\\n",
        "                                  .limit(1)\n",
        "\n",
        "# Display the result\n",
        "most_friends_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnQl7-685wxt",
        "outputId": "64235532-7e93-4045-a2e2-8a8d0c67d6a4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+\n",
            "|userid|total_friends|\n",
            "+------+-------------+\n",
            "|     1|            5|\n",
            "+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How many friend requests were sent/accepted each month?"
      ],
      "metadata": {
        "id": "t6c_8zYUL6nK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Assuming you've already created the DataFrame 'df'\n",
        "\n",
        "# Extract month and count requests\n",
        "monthly_requests_df = friend_request_df.groupBy(F.month(\"requested_on\")) \\\n",
        "                          .count() \\\n",
        "                          .withColumnRenamed(\"month(requested_on)\", \"month_num\") \\\n",
        "                          .withColumnRenamed(\"count\", \"total_requests\")\n",
        "\n",
        "# Display the results\n",
        "monthly_requests_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKj4sISCFJwR",
        "outputId": "8b6f6886-d6f6-4c9e-b86b-5121baa483d0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------+\n",
            "|month_num|total_requests|\n",
            "+---------+--------------+\n",
            "|        8|             7|\n",
            "|        7|             3|\n",
            "+---------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What percent of the total requests were accepted very next day?"
      ],
      "metadata": {
        "id": "Yc0jrUCiMQ-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Assuming you've already created the DataFrame '4rdf'\n",
        "\n",
        "# Calculate next_day_accepted column\n",
        "df = friend_request_df.withColumn(\"next_day_accepted\",\n",
        "                   F.when(F.date_add(F.col(\"requested_on\"), 1) == F.col(\"accepted_on\"), 1).otherwise(0))\n",
        "\n",
        "# Calculate the percentage\n",
        "perc_accepted_next_day = df.agg(\n",
        "    F.round(F.sum(\"next_day_accepted\") * 100.0 / F.count(\"*\"), 2).alias(\"Perc_accepted_next_day\")\n",
        ")\n",
        "\n",
        "# Display the result\n",
        "perc_accepted_next_day.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAHeKwvEMGNC",
        "outputId": "e36e85e1-22ec-4796-9350-1cea93706480"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+\n",
            "|Perc_accepted_next_day|\n",
            "+----------------------+\n",
            "|                  60.0|\n",
            "+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What percentage of users don’t have any friends?"
      ],
      "metadata": {
        "id": "gTV2cNJANc8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Assuming you've already created DataFrames for 'user_df' and 'friend_request_df'\n",
        "\n",
        "# Join tables and filter for users with no friends\n",
        "no_friends_df = user_df.alias('user').join(friend_request_df.alias(\"fr1\"), on=F.col(\"user.userid\") == F.col(\"fr1.userid\"), how=\"left\") \\\n",
        "                     .join(friend_request_df.alias(\"fr2\"), on=F.col(\"user.userid\") == F.col(\"fr2.friendid\"), how=\"left\") \\\n",
        "                     .filter(F.col(\"fr1.userid\").isNull() & F.col(\"fr2.userid\").isNull()) \\\n",
        "                     .select(\"user.userid\")\n",
        "\n",
        "# Calculate distinct count of users with no friends and all users\n",
        "no_friends_count = no_friends_df.distinct().count()\n",
        "print(no_friends_count)\n",
        "total_users_count = user_df.distinct().count()\n",
        "\n",
        "# Calculate the percentage\n",
        "perc_users_no_fr = round(no_friends_count * 100.0 / total_users_count, 2)\n",
        "\n",
        "# Display the result\n",
        "print(\"Percentage of users with no friends:\", perc_users_no_fr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvO253PsMaM2",
        "outputId": "9a243c0a-e372-4682-edaa-57d60de5b5a5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "Percentage of users with no friends: 20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gBqBEAjtNoYE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}